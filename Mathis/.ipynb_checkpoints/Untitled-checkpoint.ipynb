{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-i√®me est Mathis Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_sch\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, MSE\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def timeit(func):\n",
    "    def new_func(*args, **kwargs):\n",
    "        init_time = time()\n",
    "        res = func(*args, **kwargs)\n",
    "        print(\"{0:10s} : {1:5f}s.\".format(func.__name__, time()-init_time))\n",
    "        return res\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9998\n",
    "        self.learning_rate = 0.002\n",
    "        self.rho = 0.9\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.sequential_replay = deque([], maxlen = 500)\n",
    "        \n",
    "        input_layer = keras.Input(shape=(self.input_size,), name=\"obs\")\n",
    "        hidden = Dense(24, activation=\"elu\", kernel_initializer = \"he_uniform\")(input_layer)\n",
    "        hidden2 = Dense(24, activation=\"elu\")(hidden)\n",
    "        output_layer = Dense(self.output_size, activation=\"linear\")(hidden2)\n",
    "        self.model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        opt = RMSprop(learning_rate=self.learning_rate, rho=self.rho)\n",
    "        #opt = Adam(self.learning_rate)\n",
    "        self.optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, \"dynamic\")\n",
    "        #self.loss_func = MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
    "        self.loss_func = MSE\n",
    "        \n",
    "    def update_sequential_replay(self, state, action, reward, done, next_state):\n",
    "        self.sequential_replay.append((state, action, reward, done, next_state))\n",
    "    \n",
    "    def train(self, batch_size = 32, epochs = 1, use_loss_scale = False, use_grad_clip = False):\n",
    "        if len(self.sequential_replay) < batch_size:\n",
    "            return\n",
    "        for _ in range(epochs):\n",
    "            states, q_targets, actions = self.get_prepared_batch(batch_size)\n",
    "            self.compute_grad(states, q_targets, actions, use_loss_scale, use_grad_clip)\n",
    "    \n",
    "    def get_prepared_batch(self, batch_size = 100):        \n",
    "        batch_indices = np.random.randint(0, high = len(self.sequential_replay), size = batch_size)\n",
    "        batch_buffer = [self.sequential_replay[ind] for ind in batch_indices]                           \n",
    "        states, actions, rewards, dones, next_states = [np.array([obs[ind] for obs in batch_buffer]) for ind in range(5)]\n",
    "        \n",
    "        states = tf.constant(states, dtype=\"float32\")\n",
    "        rewards = tf.reshape(tf.constant(rewards.T, dtype=\"float32\"),(100,1))\n",
    "        next_states = tf.constant(next_states, dtype=\"float32\")\n",
    "        \n",
    "        q_targets = tf.math.add(rewards,self.gamma * tf.reduce_max(self.model(next_states), axis=1, keepdims=True))\n",
    "        return states, q_targets, actions\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_grad(self, states, q_targets, actions, use_loss_scale = False, grad_clip = 0):\n",
    "        gradients = None\n",
    "        mask = tf.one_hot(actions, self.output_size)\n",
    "        with tf.GradientTape() as tape: \n",
    "            q_value = self.model(states)\n",
    "            q_value_choose = tf.reduce_sum(mask*q_value, axis = 1, keepdims = True)\n",
    "            loss = self.loss_func(q_targets, q_value_choose)\n",
    "        if use_loss_scale:\n",
    "            scaled_loss = self.optimizer.get_scaled_loss(loss)\n",
    "            scaled_grad = tape.gradient(scaled_loss, self.model.trainable_variables)\n",
    "            gradient = self.optimizer.get_unscaled_gradients(scaled_grad)\n",
    "        else:\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        if grad_clip:\n",
    "            gradients = [tf.clip_by_norm(g, grad_clip) for g in gradients]\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        return gradients\n",
    "        \n",
    "    def get_best_action(self, state, rand=True):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if rand and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.output_size)\n",
    "        \n",
    "        state = tf.constant(state)\n",
    "        act_values = self.model(state)\n",
    "\n",
    "        action = tf.math.argmax(act_values[0]).numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_new_demand(env):\n",
    "    k = random.randrange(21)\n",
    "    env.products[\"PainDeMieEmballe\"][\"demand\"] = k\n",
    "    env.products[\"PainDeMieSansCrouteEmballe\"][\"demand\"] = 20-k\n",
    "\n",
    "@timeit\n",
    "def train(episodes, agent, env):\n",
    "    scores = list()\n",
    "    t = time()\n",
    "    for i in range(episodes+1):\n",
    "        env.reset()\n",
    "        gen_new_demand(env)\n",
    "        reward = None\n",
    "        while not env.done:\n",
    "            state = np.array([env.observation_space])\n",
    "            action = agent.get_best_action(state)\n",
    "            state, action, reward, done, next_state = env.step([action])\n",
    "            agent.update_sequential_replay(state, action[0], reward, done, next_state)\n",
    "        if i%5==0:\n",
    "            grad = agent.train(100)\n",
    "        if i%500 == 0:\n",
    "            score = 0\n",
    "            for k in range(21):\n",
    "                env.reset()\n",
    "                env.products[\"PainDeMieEmballe\"][\"demand\"] = k\n",
    "                env.products[\"PainDeMieEmballeEmballe\"][\"demand\"] = 20-k\n",
    "                while not env.done:\n",
    "                    state = np.array([env.observation_space])\n",
    "                    action = agent.get_best_action(state, rand = False)\n",
    "                    env.step([action])\n",
    "                score += env.score\n",
    "            scores.append(score/11)\n",
    "            print(\"episode: {0:5d}/{1:5d}, score: {2:6.2f}, time: {3:4.2f}s\"\n",
    "                  .format(i, episodes, scores[-1], time()-t))\n",
    "            t = time()\n",
    "            \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(scores)\n",
    "    ax.plot([0]*len(scores), color = 'r')\n",
    "    print(max(scores))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"sch-v0\")\n",
    "env.from_json(\"PraindeMine.json\")\n",
    "agent = Agent(env.observation_space.__len__(), env.action_space.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PainDeMieEmballeEmballe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-81b181af358c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-45bb8dc25772>\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0:10s} : {1:5f}s.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0minit_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-c386d8089720>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(episodes, agent, env)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mgen_new_demand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-c386d8089720>\u001b[0m in \u001b[0;36mgen_new_demand\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PainDeMieEmballe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"demand\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproducts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PainDeMieEmballeEmballe\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"demand\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PainDeMieEmballeEmballe'"
     ]
    }
   ],
   "source": [
    "train(40000, agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11):\n",
    "    env.reset()\n",
    "    env.products[\"P3\"][\"demand\"] = i\n",
    "    env.products[\"P4\"][\"demand\"] = 10-i\n",
    "    while not env.done:\n",
    "        state = np.array([env.observation_space])\n",
    "        action = agent.get_best_action(state, rand = False)\n",
    "        env.step([action])\n",
    "    print(\"Demand : {:2d}:{:2d} Score : {}.\".format(i,10-i,env.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.print_actions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.eye(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
